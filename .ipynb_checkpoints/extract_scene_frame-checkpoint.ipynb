{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d2f8f8-86bc-4adb-ad47-68adf5f24d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae75447-cc7a-4171-a8af-93dd0c82a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    packages = {\n",
    "        'scenedetect': '0.6.2',\n",
    "        'opencv-python': '4.8.1.78',\n",
    "        'timm': '0.9.12',\n",
    "        'scikit-learn': '1.3.2',\n",
    "        'ydata-profiling': '4.6.4',  \n",
    "        'plotly': '5.17.0',\n",
    "        'kaleido': '0.2.1'\n",
    "    }\n",
    "    \n",
    "    for package, version in packages.items():\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "        except ImportError:\n",
    "            os.system(f\"pip install {package}=={version} --quiet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adcd4e7d-27b6-4812-a5eb-620a1a02ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies first\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8157cd70-7262-4ca4-b1b2-5e611637abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import scenedetect\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8091fac-83e7-4480-aedf-cad4a649d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('video_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b510914-1027-48fa-9ef2-801d7f308035",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProjectConfig:\n",
    "    input_folder: str = \"test_video_frames\"\n",
    "    output_folder: str = \"test_video_frames_output\"\n",
    "    project_number: str = \"1\"\n",
    "    project_name: str = \"video_analysis\"\n",
    "    n_scene_frames: int = 3\n",
    "    image_max_size: int = 1024  # Increased for better quality\n",
    "    video_extensions: List[str] = None\n",
    "    scene_threshold: float = 30.0  # Scene detection sensitivity\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.video_extensions is None:\n",
    "            self.video_extensions = ['mp4', 'avi', 'mov', 'mkv', 'webm', 'flv', 'wmv']\n",
    "        \n",
    "        self.project_title = f\"{self.project_number}_{self.project_name}\"\n",
    "        self.project_folder = Path(self.input_folder).parent.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "812ac500-e7b8-42e3-ad4e-54a415d15022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernImageEmbedder:\n",
    "    \n",
    "    def __init__(self, model_name: str = \"efficientnet_b0\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = self._load_model()\n",
    "        self.transform = self._get_transform()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            model = timm.create_model(\n",
    "                self.model_name, \n",
    "                pretrained=True, \n",
    "                num_classes=0,  # Remove classification head\n",
    "                global_pool=''  # Remove global pooling\n",
    "            )\n",
    "            model.eval()\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {self.model_name}, falling back to resnet50\")\n",
    "            model = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "            model.eval()\n",
    "            return model\n",
    "    \n",
    "    def _get_transform(self):\n",
    "        return timm.data.resolve_data_config({}, model=self.model)\n",
    "    \n",
    "    def embed_image(self, image_path: str) -> np.ndarray:\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            # Use timm's built-in preprocessing\n",
    "            input_tensor = timm.data.transforms_factory.create_transform(**self.transform)(image)\n",
    "            input_tensor = input_tensor.unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                features = self.model(input_tensor)\n",
    "                # Global average pooling if needed\n",
    "                if len(features.shape) > 2:\n",
    "                    features = features.mean(dim=[2, 3])\n",
    "                return features.numpy().flatten()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error embedding {image_path}: {e}\")\n",
    "            return np.zeros(1000)  # Return zero vector on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e2fdd7-a39e-43e9-89de-caf934be2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernSceneDetector:\n",
    "    \n",
    "    def __init__(self, config: ProjectConfig):\n",
    "        self.config = config\n",
    "        self.embedder = ModernImageEmbedder()\n",
    "        \n",
    "    def detect_scenes(self, video_path: str) -> List[Tuple[float, float]]:\n",
    "        try:\n",
    "            # Create video manager\n",
    "            video_manager = VideoManager([str(video_path)])\n",
    "            scene_manager = SceneManager()\n",
    "            \n",
    "            # Add content detector with configurable threshold\n",
    "            scene_manager.add_detector(\n",
    "                ContentDetector(threshold=self.config.scene_threshold)\n",
    "            )\n",
    "            \n",
    "            # Detect scenes\n",
    "            video_manager.start()\n",
    "            scene_manager.detect_scenes(frame_source=video_manager)\n",
    "            video_manager.release()\n",
    "            \n",
    "            # Get scene list\n",
    "            scene_list = scene_manager.get_scene_list()\n",
    "            return [(scene[0].get_seconds(), scene[1].get_seconds()) \n",
    "                   for scene in scene_list]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Scene detection failed for {video_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_scene_frames(self, video_path: str, scenes: List[Tuple[float, float]], \n",
    "                           output_dir: str) -> pd.DataFrame:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        scene_data = []\n",
    "        frame_paths = []\n",
    "        \n",
    "        for i, (start_time, end_time) in enumerate(tqdm(scenes, desc=\"Extracting frames\")):\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            # Extract frames at evenly spaced intervals within the scene\n",
    "            frame_times = np.linspace(start_time, end_time, self.config.n_scene_frames + 2)[1:-1]\n",
    "            \n",
    "            scene_frames = []\n",
    "            for j, frame_time in enumerate(frame_times):\n",
    "                frame_number = int(frame_time * fps)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if ret:\n",
    "                    # Resize frame\n",
    "                    frame = self._resize_frame(frame)\n",
    "                    \n",
    "                    # Save frame\n",
    "                    frame_filename = f\"scene_{i:03d}_frame_{j:03d}.jpg\"\n",
    "                    frame_path = os.path.join(output_dir, frame_filename)\n",
    "                    cv2.imwrite(frame_path, frame)\n",
    "                    \n",
    "                    scene_frames.append(frame_path)\n",
    "                    frame_paths.append(frame_path)\n",
    "            \n",
    "            # Select best representative frame using embeddings\n",
    "            if scene_frames:\n",
    "                best_frame = self._select_best_frame(scene_frames)\n",
    "                \n",
    "                scene_data.append({\n",
    "                    'scene_number': i,\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'duration': duration,\n",
    "                    'representative_frame': best_frame,\n",
    "                    'all_frames': scene_frames,\n",
    "                    'frame_count': len(scene_frames)\n",
    "                })\n",
    "        \n",
    "        cap.release()\n",
    "        return pd.DataFrame(scene_data)\n",
    "    \n",
    "    def _resize_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        height, width = frame.shape[:2]\n",
    "        max_size = self.config.image_max_size\n",
    "        \n",
    "        if max(height, width) > max_size:\n",
    "            if width > height:\n",
    "                new_width = max_size\n",
    "                new_height = int(height * max_size / width)\n",
    "            else:\n",
    "                new_height = max_size\n",
    "                new_width = int(width * max_size / height)\n",
    "            \n",
    "            frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def _select_best_frame(self, frame_paths: List[str]) -> str:\n",
    "        if len(frame_paths) == 1:\n",
    "            return frame_paths[0]\n",
    "        \n",
    "        # Get embeddings for all frames\n",
    "        embeddings = []\n",
    "        for path in frame_paths:\n",
    "            embedding = self.embedder.embed_image(path)\n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        \n",
    "        # Find frame closest to centroid\n",
    "        centroid = np.mean(embeddings, axis=0)\n",
    "        similarities = cosine_similarity([centroid], embeddings)[0]\n",
    "        best_idx = np.argmax(similarities)\n",
    "        \n",
    "        return frame_paths[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebbb1dbd-b340-4b2b-b05a-59482ff09ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoProcessor:    \n",
    "    def __init__(self, config: ProjectConfig):\n",
    "        self.config = config\n",
    "        self.scene_detector = ModernSceneDetector(config)\n",
    "        self.setup_directories()\n",
    "        \n",
    "    def setup_directories(self):\n",
    "        directories = [\n",
    "            self.config.output_folder,\n",
    "            os.path.join(self.config.output_folder, \"scenes\"),\n",
    "            os.path.join(self.config.output_folder, \"metadata\"),\n",
    "            os.path.join(self.config.output_folder, \"reports\")\n",
    "        ]\n",
    "        \n",
    "        for directory in directories:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    def find_videos(self) -> List[str]:\n",
    "        \"\"\"Find all video files in input directory\"\"\"\n",
    "        video_paths = []\n",
    "        input_path = Path(self.config.input_folder)\n",
    "        \n",
    "        for ext in self.config.video_extensions:\n",
    "            video_paths.extend(input_path.rglob(f\"*.{ext}\"))\n",
    "        \n",
    "        return [str(path) for path in video_paths]\n",
    "    \n",
    "    def get_video_metadata(self, video_path: str) -> Dict:\n",
    "        \"\"\"Extract comprehensive video metadata\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        metadata = {\n",
    "            'path': video_path,\n",
    "            'name': Path(video_path).stem,\n",
    "            'extension': Path(video_path).suffix,\n",
    "            'size_mb': os.path.getsize(video_path) / (1024 * 1024),\n",
    "            'fps': cap.get(cv2.CAP_PROP_FPS),\n",
    "            'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "            'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "        }\n",
    "        \n",
    "        metadata['duration'] = metadata['frame_count'] / metadata['fps'] if metadata['fps'] > 0 else 0\n",
    "        metadata['aspect_ratio'] = metadata['width'] / metadata['height'] if metadata['height'] > 0 else 1\n",
    "        metadata['format'] = self._determine_format(metadata['width'], metadata['height'])\n",
    "        \n",
    "        cap.release()\n",
    "        return metadata\n",
    "    \n",
    "    def _determine_format(self, width: int, height: int) -> str:\n",
    "        \"\"\"Determine video format based on dimensions\"\"\"\n",
    "        ratio = width / height\n",
    "        if ratio > 1.5:\n",
    "            return \"landscape\"\n",
    "        elif ratio < 0.75:\n",
    "            return \"portrait\"\n",
    "        else:\n",
    "            return \"square\"\n",
    "    \n",
    "    def process_video(self, video_path: str) -> Dict:\n",
    "        \"\"\"Process a single video\"\"\"\n",
    "        logger.info(f\"Processing video: {Path(video_path).name}\")\n",
    "        \n",
    "        try:\n",
    "            # Get metadata\n",
    "            metadata = self.get_video_metadata(video_path)\n",
    "            \n",
    "            # Detect scenes\n",
    "            scenes = self.scene_detector.detect_scenes(video_path)\n",
    "            \n",
    "            if not scenes:\n",
    "                logger.warning(f\"No scenes detected in {video_path}\")\n",
    "                return {**metadata, 'status': 'no_scenes', 'scene_count': 0}\n",
    "            \n",
    "            # Create output directory for this video\n",
    "            video_output_dir = os.path.join(\n",
    "                self.config.output_folder, \"scenes\", metadata['name']\n",
    "            )\n",
    "            os.makedirs(video_output_dir, exist_ok=True)\n",
    "            \n",
    "            # Extract scene frames\n",
    "            scene_df = self.scene_detector.extract_scene_frames(\n",
    "                video_path, scenes, video_output_dir\n",
    "            )\n",
    "            \n",
    "            # Save scene metadata\n",
    "            scene_metadata_path = os.path.join(\n",
    "                self.config.output_folder, \"metadata\", f\"{metadata['name']}_scenes.xlsx\"\n",
    "            )\n",
    "            scene_df.to_excel(scene_metadata_path, index=False)\n",
    "            \n",
    "            # Add statistics\n",
    "            result = {\n",
    "                **metadata,\n",
    "                'status': 'success',\n",
    "                'scene_count': len(scenes),\n",
    "                'avg_scene_duration': scene_df['duration'].mean() if not scene_df.empty else 0,\n",
    "                'total_extracted_frames': scene_df['frame_count'].sum() if not scene_df.empty else 0,\n",
    "                'scenes_per_second': len(scenes) / metadata['duration'] if metadata['duration'] > 0 else 0,\n",
    "                'scene_metadata_path': scene_metadata_path\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {video_path}: {e}\")\n",
    "            metadata = self.get_video_metadata(video_path)\n",
    "            return {**metadata, 'status': f'error: {str(e)}', 'scene_count': 0}\n",
    "    \n",
    "    def process_all_videos(self) -> pd.DataFrame:\n",
    "        \"\"\"Process all videos and return results\"\"\"\n",
    "        video_paths = self.find_videos()\n",
    "        logger.info(f\"Found {len(video_paths)} videos to process\")\n",
    "        \n",
    "        if not video_paths:\n",
    "            logger.warning(\"No videos found!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        results = []\n",
    "        for video_path in tqdm(video_paths, desc=\"Processing videos\"):\n",
    "            result = self.process_video(video_path)\n",
    "            results.append(result)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save results\n",
    "        results_path = os.path.join(\n",
    "            self.config.output_folder, \n",
    "            f\"{self.config.project_title}_results.xlsx\"\n",
    "        )\n",
    "        results_df.to_excel(results_path, index=False)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def generate_report(self, results_df: pd.DataFrame):\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        if results_df.empty:\n",
    "            logger.warning(\"No data to generate report\")\n",
    "            return\n",
    "        \n",
    "        # Create visualizations\n",
    "        self._create_visualizations(results_df)\n",
    "        \n",
    "        # Generate profile report\n",
    "        profile = ProfileReport(\n",
    "            results_df,\n",
    "            title=f\"{self.config.project_title} - Video Analysis Report\",\n",
    "            explorative=True,\n",
    "            dark_mode=True\n",
    "        )\n",
    "        \n",
    "        report_path = os.path.join(\n",
    "            self.config.output_folder, \n",
    "            \"reports\",\n",
    "            f\"{self.config.project_title}_profile_report.html\"\n",
    "        )\n",
    "        profile.to_file(report_path)\n",
    "        logger.info(f\"Profile report saved to: {report_path}\")\n",
    "    \n",
    "    def _create_visualizations(self, df: pd.DataFrame):\n",
    "        \"\"\"Create custom visualizations\"\"\"\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Scene count distribution\n",
    "        df[df['scene_count'] > 0]['scene_count'].hist(bins=20, ax=axes[0,0])\n",
    "        axes[0,0].set_title('Distribution of Scene Counts')\n",
    "        axes[0,0].set_xlabel('Number of Scenes')\n",
    "        axes[0,0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Duration vs Scene count\n",
    "        valid_data = df[(df['duration'] > 0) & (df['scene_count'] > 0)]\n",
    "        axes[0,1].scatter(valid_data['duration'], valid_data['scene_count'], alpha=0.6)\n",
    "        axes[0,1].set_title('Video Duration vs Scene Count')\n",
    "        axes[0,1].set_xlabel('Duration (seconds)')\n",
    "        axes[0,1].set_ylabel('Scene Count')\n",
    "        \n",
    "        # Format distribution\n",
    "        format_counts = df['format'].value_counts()\n",
    "        axes[1,0].pie(format_counts.values, labels=format_counts.index, autopct='%1.1f%%')\n",
    "        axes[1,0].set_title('Video Format Distribution')\n",
    "        \n",
    "        # Processing status\n",
    "        status_counts = df['status'].value_counts()\n",
    "        axes[1,1].bar(range(len(status_counts)), status_counts.values)\n",
    "        axes[1,1].set_xticks(range(len(status_counts)))\n",
    "        axes[1,1].set_xticklabels(status_counts.index, rotation=45)\n",
    "        axes[1,1].set_title('Processing Status Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        viz_path = os.path.join(\n",
    "            self.config.output_folder, \n",
    "            \"reports\",\n",
    "            f\"{self.config.project_title}_visualizations.png\"\n",
    "        )\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "584c161a-c665-4a62-bc20-000d684ab1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Configuration\n",
    "    config = ProjectConfig(\n",
    "        input_folder=\"test_video_frames\",\n",
    "        output_folder=\"enhanced_video_analysis\",\n",
    "        project_name=\"modern_scene_extraction\",\n",
    "        n_scene_frames=5,  # Extract more frames per scene\n",
    "        image_max_size=1024,\n",
    "        scene_threshold=25.0  # More sensitive scene detection\n",
    "    )\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = VideoProcessor(config)\n",
    "    \n",
    "    # Process all videos\n",
    "    results_df = processor.process_all_videos()\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # Generate comprehensive report\n",
    "        processor.generate_report(results_df)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n=== Processing Summary ===\")\n",
    "        print(f\"Total videos processed: {len(results_df)}\")\n",
    "        print(f\"Successful: {sum(results_df['status'] == 'success')}\")\n",
    "        print(f\"Failed: {sum(results_df['status'] != 'success')}\")\n",
    "        print(f\"Total scenes extracted: {results_df['scene_count'].sum()}\")\n",
    "        print(f\"Average scenes per video: {results_df['scene_count'].mean():.2f}\")\n",
    "        print(f\"\\nResults saved to: {config.output_folder}\")\n",
    "    else:\n",
    "        print(\"No videos found or processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37f008a3-7c84-40e6-aaf2-44bb8bafe5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 18:01:45,114 - INFO - Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "2025-05-26 18:01:45,473 - INFO - [timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-05-26 18:01:45,521 - INFO - Found 0 videos to process\n",
      "2025-05-26 18:01:45,522 - WARNING - No videos found!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No videos found or processed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
